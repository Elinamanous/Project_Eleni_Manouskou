# Eleni Manouskou
## Project: identify the top 500 publicly traded companies by market capitalization and collect data for as many of these companies as possible on their ESG performance focusing on three broad categories: Environmental: Carbon emissions and energy usage. Social: Employee satisfaction and diversity ratios. Governance: Board composition and compliance violations.

## Part 1: Market Capitalization Data Collection
Requirements: The Python libraries used are the following: csv, requests, pandas, numpy, yfinance, concurrent.futures import ThreadPoolExecutor, time
In this specific task, I was asked to utilize financial data APIs such as Yahoo Finance or Alpha Vantage to gather data on the top 500 companies by market cap. Specifically, I was asked to categorize the data based on company name, ticker symbol, exchange, and market cap. To achieve this, I created a code in the Python programming language using the APIs from the websites mentioned earlier. Specifically, I utilized the API as stated in the documentation (In case it doesn’t work properly, an alternative API, CSV_URL_alt, can be used with the API_KEY="01ZI9U0414X5WDUW"). This key is specific to the Alpha Vantage platform and serves as a means for the API provider to track and control access to their services or resources. The APIs provided by Alpha Vantage were used to find the market cap; however, for the free version, it limits 25 API requests a day, so an alternative method was utilized.
To find the market cap of the companies that were acquired, the yfinance Python library was used, which utilizes the Yahoo API. To find the market cap, the function get_all was created, which uses the Yahoo Finance API to gather all available data for a company. The try-except method was used for cases where Yahoo Finance didn’t have information for a ticker symbol or the necessary data. After testing the function using a sample of company symbols, the function was used to find information for all the tickers on Alpha Vantage. Finally, I exported the dataset as an Excel file and as a CSV file so I could use it in the next parts.

## Part 2: ESG Data Collection
Requirements: The Python libraries used are the following: selenium, from selenium import webdriver, from selenium.webdriver.common.keys import Keys, time, requests, from bs4 import BeautifulSoup, from selenium.webdriver.common.by import By, pandas, re.

In this specific task, I was asked to collect publicly available data from freely accessible sources for companies that have to do with the ESG Data (Environmental, Social and Governance). 
Regarding the Environmental Data and the Governance Data, I couldn’t find any free relevant data collections, within the given time frame.
Regarding social data, I collected the diversity ratios of each company from the Glassdoor platform using data crawling and data mining techniques, employing BeautifulSoup, for most companies available on the site. I developed a Python script to scrape the company name (employer short names in the HTML content) and the diversity ratings from the HTML content obtained from Glassdoor web pages. Subsequently, I organized the extracted data into a pandas DataFrame for further analysis and processing. Specifically,  I created a script for web scraping to collect data from multiple pages of Glassdoor's company listings, focusing on diversity and inclusion ratings. It iterated through page numbers from 1 to 200 (inclusive) and visited each page's URL constructed with parameters like overall rating, page number, and filter type set to "RATING_DIVERSITY_AND_INCLUSION". Upon visiting each page, it retrieved the current URL and the page source, then waited for 2 seconds (using `time.sleep(2)`) to allow time for information collection. After that, it initialized empty lists for company names and reviews. Next, it parsed the page source using BeautifulSoup to extract relevant content, specifically looking for elements with the class "col-md-8" and appending them to the list named `all_contents’. Then, an empty list named "frames" was initialized, which was utilized to store the scraped data frames. A code was created to iterate through each HTML content stored in the "all_contents" variable. Within the loop, it executed the following tasks for each HTML content. Firstly, it converted the HTML content into a string named "html_string". Then, it defined regular expression patterns using the "re" module to extract specific information from the HTML content, designed to extract company names (employer short names), ratings, and rating types (such as Diversity & Inclusion). Subsequently, it applied the defined patterns using "re.findall()" to extract matches from the HTML content. The extracted matches represented employer short names, ratings, and rating types. To filter the diversity ratings from the extracted ratings, it iterated through each rating, matching it with the corresponding rating type. Ratings categorized under "Diversity & Inclusion" were stored in the "div" list. Next, it constructed a pandas DataFrame (df) containing the extracted employer short names and filtered diversity ratings. This DataFrame was then appended to the "frames" list. Finally, after iterating through all HTML contents, the code concatenated all DataFrames stored in the "frames" list into a single DataFrame named "result". The index of the "result" DataFrame was reset, and it was displayed, as seen in the results. Finally, I exported the dataset as an Excel file and as a CSV file so I could use it in the next parts.


## Part 3: Data Cleaning and Transformation

Requirements: The Python libraries used are the following: pandas, from fuzzywuzzy import process.
In this specific task, I was asked to Clean the data to ensure accuracy and consistency, align company dentifiers across datasets, handle missing values thoughtfully, ensure data formats are standardized for analysis and merge the market cap data with ESG metrics into a single dataset. 
As mentioned earlier, I was only able to find the social data for companies in part 2. Therefore, I merged the market cap data from part-1 with the social data from part-2. Specifically, I developed a Python code that reads the two CSV files created in parts 1 and 2 and names them "df1" for the company ratings (part-2) and "df2" for the top 500 market cap companies (part-1). I wanted to merge these two datasets, which means the code must combine the "longName" list, containing the companies, with other lists (rating, symbol, marketCap, and exchange) to create a single dataset. This final dataset will include only the companies that exist in both datasets. After reviewing the two files, I noticed that although they have the same companies (listed under "longName" in the CSV files), in the "top_500_market_cap_comps" file, the companies have their official names (such as Amazon.com, Inc), whereas in the "company_ratings" file, the company names are simplified (such as Amazon). The potential issue is that if I simply merge these two datasets, the code may not recognize the names from "df1" dataset as being the same as the names from "df2" because they are not exactly the same. For that reason, I first created a code that searches in the "df2" file to see which longName columns contain the same words as the columns in "df1". More specifically, an empty list named merged_dfs is initialized, which will be used to store the merged DataFrames. Then, the code iterates through each value in the 'longName' column of the first DataFrame (df1). For each value (word), it selects rows from the second DataFrame (df2) where the 'longName' column contains that word (case insensitive). If there are rows in df2 that match the current word, those rows are appended to the merged_dfs list. After iterating through all words in the 'longName' column of df1, the code concatenates all DataFrames stored in the merged_dfs list into a single DataFrame named final_data_set which contains only the companies that excist both in df1 and df2 datasets. Then I drop the duplicates. Finally, I created a function that preprocesses a given longName by removing specific strings from it and takes a long name (long_name) as input. It defines a list named strings_to_remove containing strings that need to be removed from the longName. These strings typically include common company suffixes or identifiers like 'Inc.', 'Corporation', 'Company', 'plc', 'Incorporated', and '.com'. Then, the function iterates over each string in the strings_to_remove list. For each string, it removes occurrences of that string from the long name using the replace() method and then strips any leading or trailing whitespace from the result. Finally, it is applied to the 'longName' column of the DataFrame final_data_set using the apply() method. The preprocessed long names are stored in a new column named 'processed_long_name. The result is a dataset that contains the common companies with the market cap data and the ratings.

## Part 4: Data Analysis and Visualization
I wasn’t able to complete the task within the given time frame



